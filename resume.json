{"skills":[],"basics":{"name":"Tools ","email":["jyothi23jiya@gmail.com"]},"education":[],"work":[{"summary":" \u2013 Present\n\nSenior Informatica Developer\n\nResponsibilities:\n\n· Involved in design, development and maintenance of database for Data warehouse project.\n\n· Involved in Business Users Meetings to understand their requirements.\n\n· Converted business requirements into technical documents- BRD, explained business requirements in terms of technology to the developers.\n\n· Used of Kettle for data integration delivers powerful extraction, transformation and loading (ETL) capabilities.\n\n· Created IBM DB2 that contain database server and support the relational model and used   object-relational  features and non-relational structures like JSON and XML.\n\n· Used of Business Object to provide common services for deployment and management tools of BI tools.\n\n· BO is used for integration of data from Business Intelligence .\n\n· Used of different tools of Kettle in software like GeoKettle ETL , JasperSoft ETL etc.\n\n· Used of DB2 as a MVC frame and for traditional product packaging.\n\n· Used of talend to combines data preparation and data integration into a single unified platform to transform\n\n· Talend provide  self-service tools to catalog, cleanse, and shape data from any source for use anywhere to  the use of data and facilitates collaboration across batch, bulk, and master data management scenarios.\n\n· Installed MDM and used to enterprise to link all of its critical data to one file that provides a common point of reference.\n\n· Used of Kettle java based architecture and open XML based configuration, It included support for integration of security and data management tools.\n\n· Used of Mongo DB as a open source software avoids the traditional table-based  relational database  structure in favor of  JSON -like documents with dynamic schemas  (MongoDB calls the format  BSON ).\n\n· Developed Object system for a particular attribute or entity that involved class and class based objects.\n\n· Mongo DB provide the support to regular expression searches. Queries can return specific fields of documents ,  range queries and  include user-defined  JavaScript  functions\n\n· Installed and configured Hadoop MapReduce, HDFS, developed multiple MapReduce jobs in java for data cleaning and pre- processing.\n\n· Loaded home mortgage data from the existing DWH tables (SQL Server) to HDFS using Sqoop.\n\n· Developed Data Flow diagrams to create Mappings and Test plans.\n\n· Populated HDFSand Cassandra with huge amounts of data using Apache\n\n· The Data flow diagrams ranged from OLTP systems to staging to Data warehouse.\n\n· Developed Test plan to verify the logic of every Mapping in a Session. The test plans included counts\n\nverification, look up hits, transformation of each element of data, filters, and aggregation and target.\n\n· Developed complex Informatica mappings using various transformations- Source Qualifier, Normalizer,\n\n· Used of Dataflux to dataintegration , migration and consolidation to data quality .\n\n· Used of Dataflux tolls , Its Data management studio that provides a unified development and delivery environment to provide a single point of control to manage data quality, data integration, master data management (MDM) and other data initiatives.\n\n· Used of Infosphere MDM to design functions and web services .\n\n· Worked to manage of IBM Infosphere master data for single or multiple domains \u2013 customers, patients, citizens, suppliers, locations, products, services offerings, accounts.\n\n· Created Informatica Power Exchange Registration and Data map.\n\n· Created scripts to create new tables, views, queries for new enhancement in the application using TOAD.\n\n· Designed and developed real-time data warehouse.\n\n· Involved in implementing the Land Process of loading the customer\/product Data Set into Informatica MDM Hub using ETL tool that was coming from various source systems.\n\n· Worked on Informatica cloud and extracted data from Sales Force source.\n\n·  Implemented Pentaho based reporting and Ad-hoc reporting analysis platform\n\n· Worked on Teradata utilities BTEQ, MLOAD, FLAOD and TPUMP to load staging area.\n\n· Data load from file to Hadoop cluster, Big Data summarization using Hive (using Pentaho) and loading of summarized data into Oracle and MYSQL for data visualizations and ad-hoc reporting\n\n· Worked on Audit login, Error login and Reference check while loading data to Data warehouse.\n\n· Created Unix Script for ETL jobs, session log cleanup and dynamic parameter.\n\n· Performed Unit testing, Integration testing and System testing of Informatica mappings.\n\n· Wrote Unix scripts, Perl scripts for the business needs.\n\n· Coded Unix Scripts to capture data from different relational systems to flat files to use them as source file\n\nfor ETL process.\n\nEnvironment: Informatica power Center 9.5\/9.1, Informatica Cloud ,Kettle, IDQ, MongoDB , HDFS , DB2, Informatica Power Exchange 9.1,HDFS,HQLMetadata editor, Talend , Tidal, Shell, IBM MDM 8.5,Scripting,pentaho, Perl Scripting, Windows XP, Putty info sphere , Dataflux MDM, DB2 Mainframe, MDM Consultant, , Informatica MDM OBIEE 11g, Oracle Exadata, ","company":"OCZ Technology","startDate":"April 2015"},{"summary":" R2, T-SQL, Erwin 8, SQL Server 2008, TOAD,  Pentaho Kettle, PL\/SQL Developer, Linux, Unix.\n\nBelo Corporation , Dallas , Tx January 2014 \u2013 March 2015\n\nETL Developer\n\nResponsibilities:\n\n· Used Informatica power Center for extracting source data and loading into target table\n\n· Involved in complete life cycle implementation of Informatica MDM 9.1 implementation for Customer Master.\n\n· Used of SAP Sybase designer and its features data modeling software and metadata management solution for data architecture, information architecture and enterprise architecture.\n\n· Used of Power Designer that brings powerful impact analysis, design-time change management and metadata management techniques.\n\n· Provide the data integration platform to use of kettle , integrate of different data sources for building and updating geospatial databases , data warehouse and services.\n\n· Used of Talend software for big data integration,  Cloud , data integration ,  data management ,  Master Data Management , data quality, data preparation and enterprise application integration .\n\n· It provides a common set of application and data integration tools to build a service-oriented architecture, and connect, mediate, and manage services in real-time.\n\n· Installed, configured and tuned Informatica\u2019s Data Quality Tool and implemented Informatica\u2019s Data Quality applications for both business and technology users.\n\n· Used of DQ standard, Data Profiling guide lines, Process Sequence, Dictionaries, DQ Lifecycles, Naming Convention and Version Control for different tools.\n\n· Designed Python to have an uncluttered visual layout, often using English keywords where other languages use punctuation.\n\n· Used of Python to support multiple programming imperative and functional programming or procedural styles .\n\n· Performed IDQ at the point of entry for each mandatory attribute from each source system, attribute values are created way after the initial creation of the transaction.\n\n· Involved IBM Info sphere integrate MDM into existing business processes and technical architectures.\n\n· Used of OS(object System) to manage the group of different software\u2019s and networks.\n\n· Transferred the data from database management system to another file to used of Kettle .\n\n· Worked on  Informatica\u2019s Data Quality as a service in a SOA or Web Services architecture .\n\n· Developed confidence factor scoring system, Run the Scorecard, Edit the Scorecard, Configure Thresholds and designed Score Carding patterns use of Informatica DQ.\n\n· Used DB2 on cloud and for its integration of data and for data storage , it\u2019s a tool of DBMS\n\n· Installed of Cloud and Salesforce and all the tools of Cloud for integration .\n\n· Used of Informatica Cloud is easily connect to variety of cloud on-premises, mobile and social data sources.\n\n· Used of Cloud Info sphere fordevelopment and testing environments and even move some of existing workloads to the cloud without impacting production systems.\n\n· Cloud involved beyond traditional point-to-point integration vendors by offering the most complete suite of cloud integration for batch and real time patterns, cloud test data management, cloud data quality and cloud master.\n\n· Installed Informatica MDM platform on Web logic Application Server and did the configuration.\n\n· Designed and Developed ETL processes using data stage designer to load data into oracle database.\n\n· Created Unix Shell scripts for FTP\/MFT, Error handling, Error reports, Parameter files etc.\n\n· Configured the PLM for Process application in accordance to the Kroger\u2019s requirements.\n\n· Built project plans and developed detailed data migration process documentations for KMP and KLP projects.\n\n· Worked on column level data validation rules on the source data before the data enters the records by record of data stage of formatting and standardization. \n\n· Used DataStage Manager to implement the Import and Export Interfaces.\n\n· Configured PLM for Process Core data to maintain data list including Global Specification Management (GSM) common data, GSM compliance data, New Product Development (NPD) data, Supply Chain Relationship Management (SCRM) data, and Workflow Administration (WFA) data.\n\n· Created Extended Attributes that captures textual, numeric, Boolean, calculated, date, and range data to support client\u2019s inclusive requirements.\n\n· Performed the ETL coding using Informatica Power center 9.1 (Designer, Workflow Manager, Workflow Monitor) and B2B.\n\n· Installation\/Config\/Upgrade of Fusion Middleware Web logic (WSL), Discoverer 11g and configuring it with Oracle EBS R12.1.3.\n\n· Created Pentaho Jobs and transformations to load data from CSV, Excel files to MYSQL database \n\n· Created Jobs and transformations in Pentaho Kettle to move Oracle data pump files \n\n· Configured and Installed Oracle RAC 10g & Grid Control 10g on RHAS 4.0 - for applications based on Java OLTP.\n\n· Track and report upon testing activities, including the test case execution of data stage, defect status if any defects opened during execution and the testing results status. \n\n· Performed the Informatica code review and ETL design review of other team members.\n\n· Managed Custom data while creating extended attributes to capture custom information and elements that are unique to client\u2019s business.\n\nUsed SSIS packages to move logins and jobs from one server to another server.\n\n· Demonstrated out-of-box reports provided within the OPLA solution.\n\nEnvironment: Oracle RDBMS 10g, OBIA\/BI APPS 7.9.6\/7.9.5, Talend, Cloud, Python ,SAP Sybase and features, Salesforce and Oracle 11g ,SSIS,IBM DB2, HDFS,Spark , Kafka, Kettle,IDQ, Informatica 9.1\/8.6,Datastage Enterprise 9.1\/8.0\/7.5,Windows Server ","company":"Teradata","startDate":"2008"},{"summary":" Info sphere, Pentaho,Linux,Unix, Pentaho Kettle, Pentaho Metadata editor ,Informatica MDM Agile Product Lifecycle Management (PLM) for Process 6.0, Unix Shell Scripting,PL\/SQL,Oracle Business Intelligence Enterprise Edition (OBIEE) 10.1.3.x,Netezza, OBIEE (11.1.1.6.9), Oracle Product Lifecycle Analytics (OPLA) 3.0, Verdant Data Loader 6.0, ETL. \n\nInfo beans, Indore, India July 2011 \u2013 July 2013\n\nInformatica Developer\n\nResponsibilities:\n\n· Interacting with Business and analysts to create the Functional Specs.\n\n· Developing reports and intelligent dashboards for Global Sales team.\n\n· Performance tuning required for slow running reports, designing of performance enhancing structures on Database.\n\n· Used of Cloud tools for integration of data and databases, it provide easy interface to access the data and retry.\n\n· Created Teradata External loader connections such as MLoad, Upsert and Update, Fastload while loading data into the target tables in Teradata Database. \n\n· POCs for implementing Informatica scheduling on Job Automation tool UC4 and making ETL loads as flexible and restart able.\n\n· Used of Autosys Software , It scheduled the job using of unix and linux.\n\n· Reviewing and creating the design required to support all reporting needs.\n\n· Getting signoff on report and dashboard template.\n\n· Optimized the SSRS reports through the use of aggressive scoping of data and judicious use of aggregate tables and materialized views and Caching techniques. \n\n· Used nested stored procedures with complex control flow logic to feed SSRS reports. \n\n· Reverse Engineering the Data Model from Erwin.\n\n· Involved in creating Data stage tables\/mappings\/ setting the trust level -HDD in Siperian for Customer Master system. \n\n· Use of Python scripting language  for  web applications ,for the Apache Server and   Web Server Gateway Interface , a standard API has evolved to facilitate these applications.\n\n· Designed API to describe the principal of Information hiding role of programming interfaces as enabling  modular programming by hiding the implementation details of the modules.\n\n· Used of CAPI for develop a program  by providing all the building blocks for  for a web-based system, operating system, database system,  computer hardware , or  software library .\n\n· Used the metadata of Informatica repository tables\n\n· Proficient in creating SQL Server reports, handling sub reports and defining query for generating drill down reports and drill through reports using SSRS 2005\/2008.\n\n· SSIS Packages are created to export and import data from CSV files, Text files and Excel Spreadsheet.\n\n· Used IBM Web SphereData Stage for data quality. Setting up rules, best practices, and managing project resources. \n\n· Developing reports and intelligent dashboards for Global Sales team.\n\n· Performance tuning required for slow running reports, designing of performance enhancing structures on Database.\n\n· Created agents using Obiee delivers to send emails in case of ETL load failures and for long running jobs.\n\n· Used statistical functions like regression to view the performance trends.\n\n· Created Teradata External loader connections such as MLoad, Upsert and Update, Fastload while loading data into the target tables in Teradata Database. \n\n· POCs for implementing Informatica scheduling on Job Automation tool UC4 and making ETL loads as flexible and restart able.\n\n· Coordinated with Database administrator team to make sure database gets executed correctly at data stage and production instances before loads can start. \n\n· Modeled Obiee RPD to use Informatica Repository tables to generate reports for ETL loads\n\n· For tracking the current status of loads, current\/historic performance, throughput, differentiating long running jobs, view performance trends for individual Informatica sessions and developed metrics to view ETL performance.\n\n· Created agents using obiee delivers to send emails in case of ETL load failures and for long running jobs.\n\n· Used statistical functions like regression to view the performance trends.\n\nEnvironment: (Oracle Business Intelligence Suite) OBIEE 11.1.6\/11.1.5, Informatica Power center, Informatica Cloud ,Informatica Multi-domain MDM 9.1,SSIS,SSRS,Informatica B2B DX DT v 8.0, Oracle 10g\/11g, BI Publisher, TOAD, Python, API ,Teradatav12, PL\/SQL,SQL, SQL Server 2000\/2005, Red Gate , Windows 7, Unix, Linux and Windows 2003\n\nDeloitte, Hyderabad August 2009 \u2013 June 2011\n\nETL Developer                     \n\nResponsibilities:\n\n· Developed Metadata repository (rpd), configured metadata the Presentation Layer, Business Model Layer & Physical Data Model.\n\n· Designed developed and tested complex ETL for application integration using IBM - Info Sphere Data Stage\n\n· Provide suggestions to ETL team for performance improvement.\n\n· Testing and reviewing the RPD\n\n· Used different features of Sybase power designer and DBAs with robust heterogeneous support for all leading databases, and brings impact analysis and design-time change management together with formal database design techniques.\n\n· It involved Sybase power Designer provides complete modeling for information architecture and unequaled traceability from data source to warehouse or mart and one cohesive, integrated metadata repository.\n\n· Most important reports are sourced from Portfolio Subject Area to manage the overall portfolio risk and sector concentrations.\n\n· Deployed SSRS reports to the reporting server and assisted in troubleshooting deployment problems. \n\n· IBM data quality tools of Data Stage were used to help with the cleansing for migration.\n\n· Responsible for the management and migration of SSIS packages in the staging and pre-production environments. \n\n· OBIEE Administration activities, RPD and web catalog deployment to ST, UAT and production.\n\n· Supporting UAT and fixing UAT trackers.\n\n· Implementation to production and supporting business checkout.\n\n· Constant interactions with the DBA and Infrastructure support teams to maintain dev\/test environments space requirements and access issues.\n\n· Generated Dynamic reports from Cubes using Report Builder and SSRS. \n\n· Developed Variance reports using Reporting Services (SSRS) \n\n· Study and comprehend business requirements, constant interaction with business and come up with a detailed technical specification document.\n\nEnvironment : Erwin, OBIEE 10.1.3.4.0, Teradata, Java ,C,Toad, Data Stage,Informatica 8.1.1, Sun Solaris, Windows, SSIS, SSRS ,Oracle 10g ,Clear Case, Sybase power Design, Unix Shell Scripting.\n\nTECHNICAL SKILLS:\n\nInformatica Power Center 9.1,9.5\/8.x\/7.x\/6.x\/5., FACT and dimension tables, Erwin 4.0, Business Objects, Erwin 4.x, C\/C++, Java SE 1.4, , VB.Net, HTML, SQL Server, MySQL, Pentaho, InformaticaMDM, Oracle 8i,TOAD, Sybase Power Designer (v4 \u201311), Sybase System 10, Exadata, 1,Informatica Power exchange ,ERWIN, ETL: Data Stage (7.x, 6.x, 5.x), Star-Schema Modelling, Kettle, Snowflake Modelling Sybase ASE (11 - 12) and Sybase ASA (6 - 9).ASP\/ASP.net, B2B DT 9.5, B2B DX 9.5,IDQ, Unix, MVS, MS Windows\/95\/98\/NT\/2000\/XP,,LINUX,MS DOS, MS Office 2000,Informatica 9.0.1, 8.1, 7.1,Oracle 11g, 10g,Teradata ,PL\/SQL , IBM MDM 8.5 , Pentaho Kettle, Pentaho Analyzer, MDM Consultant,OBIEE 11.1.1.5.4, 10.1.3.4 , B2B Data Transformation Studio.\n\nEDUCATION DETAILS:\n\nMaster of Computer Science, Fairleigh Dickinson University, NJ\n\nBachelor of Engineering in Computer Science, Anna University, Chennai.\n","company":"IBM","startDate":"2003"}]}